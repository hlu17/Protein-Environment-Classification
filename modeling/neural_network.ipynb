{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d4965eb",
   "metadata": {},
   "source": [
    "# Example modeling process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10664d32",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56cae12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from helpers import (get_training_observations, \n",
    "                     get_training_labels, \n",
    "                     get_protein_proportions,\n",
    "                     drop_empty_columns)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Model specific imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02264c03",
   "metadata": {},
   "source": [
    "### Load training data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "426cf755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting all training observations from 'metagenome_classification.db'...\n",
      "There are 16306 features\n",
      "Getting all training labels from 'metagenome_classification.db'...\n"
     ]
    }
   ],
   "source": [
    "x_train_raw_counts = get_training_observations()\n",
    "x_train = get_protein_proportions(x_train_raw_counts)\n",
    "print(f\"There are {x_train.shape[1]} features\")\n",
    "y_train = get_training_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f0907b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>index</th>\n",
       "      <th>PF00001.19</th>\n",
       "      <th>PF00002.22</th>\n",
       "      <th>PF00003.20</th>\n",
       "      <th>PF00004.27</th>\n",
       "      <th>PF00005.25</th>\n",
       "      <th>PF00006.23</th>\n",
       "      <th>PF00007.20</th>\n",
       "      <th>PF00008.25</th>\n",
       "      <th>PF00009.25</th>\n",
       "      <th>PF00010.24</th>\n",
       "      <th>...</th>\n",
       "      <th>PF17216.1</th>\n",
       "      <th>PF17217.1</th>\n",
       "      <th>PF17218.1</th>\n",
       "      <th>PF17219.1</th>\n",
       "      <th>PF17220.1</th>\n",
       "      <th>PF17221.1</th>\n",
       "      <th>PF17222.1</th>\n",
       "      <th>PF17223.1</th>\n",
       "      <th>PF17224.1</th>\n",
       "      <th>PF17225.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004020</td>\n",
       "      <td>0.006243</td>\n",
       "      <td>0.001039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003575</td>\n",
       "      <td>0.013739</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002235</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.376880e-07</td>\n",
       "      <td>2.188440e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001619</td>\n",
       "      <td>0.016218</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.001711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002737</td>\n",
       "      <td>0.019874</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001992</td>\n",
       "      <td>0.012389</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.016435</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001806</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.023274</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002301</td>\n",
       "      <td>0.015752</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.014797</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004574</td>\n",
       "      <td>0.013601</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002402</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 16306 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "index    PF00001.19    PF00002.22  PF00003.20  PF00004.27  PF00005.25  \\\n",
       "0      0.000000e+00  0.000000e+00         0.0    0.004020    0.006243   \n",
       "1      0.000000e+00  0.000000e+00         0.0    0.003575    0.013739   \n",
       "2      4.376880e-07  2.188440e-07         0.0    0.001619    0.016218   \n",
       "3      0.000000e+00  0.000000e+00         0.0    0.002737    0.019874   \n",
       "4      0.000000e+00  0.000000e+00         0.0    0.001992    0.012389   \n",
       "5      0.000000e+00  0.000000e+00         0.0    0.001684    0.016435   \n",
       "6      0.000000e+00  0.000000e+00         0.0    0.001404    0.023274   \n",
       "7      0.000000e+00  0.000000e+00         0.0    0.002301    0.015752   \n",
       "8      0.000000e+00  0.000000e+00         0.0    0.001885    0.014797   \n",
       "9      0.000000e+00  0.000000e+00         0.0    0.004574    0.013601   \n",
       "\n",
       "index  PF00006.23  PF00007.20  PF00008.25  PF00009.25  PF00010.24  ...  \\\n",
       "0        0.001039         0.0    0.000000    0.003265    0.000000  ...   \n",
       "1        0.001026         0.0    0.000000    0.002235    0.000007  ...   \n",
       "2        0.000916         0.0    0.000002    0.001711    0.000000  ...   \n",
       "3        0.001785         0.0    0.000000    0.003570    0.000000  ...   \n",
       "4        0.001154         0.0    0.000013    0.002286    0.000000  ...   \n",
       "5        0.000931         0.0    0.000000    0.001806    0.000000  ...   \n",
       "6        0.000845         0.0    0.000000    0.001497    0.000000  ...   \n",
       "7        0.000925         0.0    0.000000    0.001843    0.000000  ...   \n",
       "8        0.001012         0.0    0.000000    0.001689    0.000000  ...   \n",
       "9        0.000995         0.0    0.000000    0.002402    0.000000  ...   \n",
       "\n",
       "index  PF17216.1  PF17217.1  PF17218.1  PF17219.1  PF17220.1  PF17221.1  \\\n",
       "0            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "1            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "2            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "3            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "4            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "5            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "6            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "7            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "8            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "9            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "\n",
       "index  PF17222.1  PF17223.1  PF17224.1  PF17225.1  \n",
       "0            0.0        0.0        0.0        0.0  \n",
       "1            0.0        0.0        0.0        0.0  \n",
       "2            0.0        0.0        0.0        0.0  \n",
       "3            0.0        0.0        0.0        0.0  \n",
       "4            0.0        0.0        0.0        0.0  \n",
       "5            0.0        0.0        0.0        0.0  \n",
       "6            0.0        0.0        0.0        0.0  \n",
       "7            0.0        0.0        0.0        0.0  \n",
       "8            0.0        0.0        0.0        0.0  \n",
       "9            0.0        0.0        0.0        0.0  \n",
       "\n",
       "[10 rows x 16306 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc49d270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>index</th>\n",
       "      <th>EMPO_1</th>\n",
       "      <th>EMPO_2</th>\n",
       "      <th>EMPO_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Free-living</td>\n",
       "      <td>Saline</td>\n",
       "      <td>Hypersaline (saline)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Free-living</td>\n",
       "      <td>Saline</td>\n",
       "      <td>Water (saline)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Host-associated</td>\n",
       "      <td>Plant</td>\n",
       "      <td>Plant rhizosphere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Free-living</td>\n",
       "      <td>Non-saline</td>\n",
       "      <td>Soil (non-saline)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Free-living</td>\n",
       "      <td>Saline</td>\n",
       "      <td>Water (saline)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "index           EMPO_1      EMPO_2                EMPO_3\n",
       "0          Free-living      Saline  Hypersaline (saline)\n",
       "1          Free-living      Saline        Water (saline)\n",
       "2      Host-associated       Plant     Plant rhizosphere\n",
       "3          Free-living  Non-saline     Soil (non-saline)\n",
       "4          Free-living      Saline        Water (saline)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f397441",
   "metadata": {},
   "source": [
    "### Data manipulation\n",
    "Validation splits, dimensionality reduction, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eac6bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/validation if not CV\n",
    "X_tr, X_val, Y_tr, Y_val = train_test_split(x_train, y_train, test_size=0.2) #, random_state=1)\n",
    "\n",
    "# Dimensionality reduction?\n",
    "svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42)\n",
    "svd.fit(X_tr)\n",
    "\n",
    "new_x_train = svd.transform(X_tr)\n",
    "new_x_val = svd.transform(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c051696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string labels to numeric\n",
    "labels3 = [\n",
    "    'Aerosol (non-saline)',\n",
    "    'Animal corpus',\n",
    "    'Animal proximal gut',\n",
    "    'Hypersaline (saline)',\n",
    "    'Plant corpus',\n",
    "    'Plant rhizosphere',\n",
    "    'Plant surface',\n",
    "    'Sediment (non-saline)',\n",
    "    'Sediment (saline)',\n",
    "    'Soil (non-saline)',\n",
    "    'Subsurface (non-saline)',\n",
    "    'Surface (non-saline)',\n",
    "    'Surface (saline)',\n",
    "    'Water (non-saline)',\n",
    "    'Water (saline)'\n",
    "]\n",
    "labels3_map = {}\n",
    "for i in range(0,len(labels3)):\n",
    "    label = labels3[i]\n",
    "    labels3_map[label] = i\n",
    "\n",
    "Y_tr['EMPO_3_int'] = Y_tr['EMPO_3'].map(labels3_map)\n",
    "Y_tr.head()\n",
    "Y_val['EMPO_3_int'] = Y_val['EMPO_3'].map(labels3_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bb1981",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22269d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-11 00:59:29.503000: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 1s 69ms/step - loss: 2.2393 - accuracy: 0.2503 - val_loss: 1.8413 - val_accuracy: 0.4100\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.7266 - accuracy: 0.4327 - val_loss: 1.4680 - val_accuracy: 0.5000\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 1.3343 - accuracy: 0.5506 - val_loss: 1.3498 - val_accuracy: 0.6000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 1.2228 - accuracy: 0.5573 - val_loss: 1.1066 - val_accuracy: 0.6900\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.9920 - accuracy: 0.6752 - val_loss: 0.7896 - val_accuracy: 0.7600\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.8505 - accuracy: 0.7219 - val_loss: 0.6827 - val_accuracy: 0.8200\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.7525 - accuracy: 0.7531 - val_loss: 0.6623 - val_accuracy: 0.7900\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.6853 - accuracy: 0.7842 - val_loss: 0.6159 - val_accuracy: 0.7800\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.7013 - accuracy: 0.7631 - val_loss: 0.6426 - val_accuracy: 0.7900\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.6011 - accuracy: 0.8053 - val_loss: 0.6121 - val_accuracy: 0.7900\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.5758 - accuracy: 0.8142 - val_loss: 0.7560 - val_accuracy: 0.7200\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 49ms/step - loss: 0.7555 - accuracy: 0.7286 - val_loss: 0.6546 - val_accuracy: 0.8100\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.5620 - accuracy: 0.7942 - val_loss: 0.6745 - val_accuracy: 0.8200\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.5678 - accuracy: 0.8309 - val_loss: 0.4345 - val_accuracy: 0.8700\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.4882 - accuracy: 0.8331 - val_loss: 0.6278 - val_accuracy: 0.7800\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.5288 - accuracy: 0.8198 - val_loss: 0.5959 - val_accuracy: 0.8700\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.5723 - accuracy: 0.8154 - val_loss: 0.7681 - val_accuracy: 0.8000\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.5985 - accuracy: 0.7998 - val_loss: 0.8852 - val_accuracy: 0.7100\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 32ms/step - loss: 0.6478 - accuracy: 0.7898 - val_loss: 0.7241 - val_accuracy: 0.7700\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.6108 - accuracy: 0.8131 - val_loss: 0.4539 - val_accuracy: 0.8900\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.4803 - accuracy: 0.8521 - val_loss: 0.5908 - val_accuracy: 0.8300\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.4058 - accuracy: 0.8587 - val_loss: 0.5027 - val_accuracy: 0.8900\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.2982 - accuracy: 0.9010 - val_loss: 0.5284 - val_accuracy: 0.8700\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.4276 - accuracy: 0.8476 - val_loss: 0.4580 - val_accuracy: 0.8100\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.4087 - accuracy: 0.8687 - val_loss: 0.4238 - val_accuracy: 0.9100\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.3791 - accuracy: 0.8643 - val_loss: 0.6950 - val_accuracy: 0.8400\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.4790 - accuracy: 0.8398 - val_loss: 0.5341 - val_accuracy: 0.8200\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.4625 - accuracy: 0.8554 - val_loss: 0.5308 - val_accuracy: 0.8200\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.3376 - accuracy: 0.8988 - val_loss: 0.3744 - val_accuracy: 0.9000\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.2588 - accuracy: 0.9166 - val_loss: 0.3917 - val_accuracy: 0.8900\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.2300 - accuracy: 0.9266 - val_loss: 0.3463 - val_accuracy: 0.9100\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.1922 - accuracy: 0.9466 - val_loss: 0.3443 - val_accuracy: 0.8900\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.3210 - accuracy: 0.8865 - val_loss: 0.5474 - val_accuracy: 0.8700\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.2451 - accuracy: 0.9143 - val_loss: 0.4237 - val_accuracy: 0.9300\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.1977 - accuracy: 0.9388 - val_loss: 0.6467 - val_accuracy: 0.8200\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.4913 - accuracy: 0.8509 - val_loss: 0.5881 - val_accuracy: 0.8600\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.3510 - accuracy: 0.8821 - val_loss: 0.4121 - val_accuracy: 0.8900\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.2966 - accuracy: 0.9088 - val_loss: 0.4492 - val_accuracy: 0.9000\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.2231 - accuracy: 0.9277 - val_loss: 0.3678 - val_accuracy: 0.8900\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.2013 - accuracy: 0.9277 - val_loss: 0.4717 - val_accuracy: 0.8500\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.5245 - accuracy: 0.8354 - val_loss: 0.5155 - val_accuracy: 0.8700\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.3694 - accuracy: 0.8832 - val_loss: 0.7077 - val_accuracy: 0.8000\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.2804 - accuracy: 0.9043 - val_loss: 0.4424 - val_accuracy: 0.8900\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.2048 - accuracy: 0.9266 - val_loss: 0.3492 - val_accuracy: 0.9100\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.1686 - accuracy: 0.9499 - val_loss: 0.3784 - val_accuracy: 0.9000\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.1528 - accuracy: 0.9544 - val_loss: 0.2600 - val_accuracy: 0.9400\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.1493 - accuracy: 0.9544 - val_loss: 0.3560 - val_accuracy: 0.9200\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.1261 - accuracy: 0.9644 - val_loss: 0.2829 - val_accuracy: 0.9200\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.1170 - accuracy: 0.9655 - val_loss: 0.3412 - val_accuracy: 0.9400\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.1123 - accuracy: 0.9711 - val_loss: 0.2970 - val_accuracy: 0.9400\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.1056 - accuracy: 0.9700 - val_loss: 0.3560 - val_accuracy: 0.9100\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.1492 - accuracy: 0.9600 - val_loss: 0.3232 - val_accuracy: 0.9000\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.1442 - accuracy: 0.9555 - val_loss: 0.3385 - val_accuracy: 0.9100\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.1140 - accuracy: 0.9711 - val_loss: 0.3464 - val_accuracy: 0.9400\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.1150 - accuracy: 0.9689 - val_loss: 0.2981 - val_accuracy: 0.9400\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.0995 - accuracy: 0.9677 - val_loss: 0.3485 - val_accuracy: 0.9100\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.0927 - accuracy: 0.9744 - val_loss: 0.3157 - val_accuracy: 0.9400\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.0926 - accuracy: 0.9722 - val_loss: 0.3199 - val_accuracy: 0.9500\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 39ms/step - loss: 0.0847 - accuracy: 0.9755 - val_loss: 0.3255 - val_accuracy: 0.9400\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.0883 - accuracy: 0.9766 - val_loss: 0.3422 - val_accuracy: 0.9400\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.0988 - accuracy: 0.9744 - val_loss: 0.3371 - val_accuracy: 0.9500\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 0.0888 - accuracy: 0.9778 - val_loss: 0.2925 - val_accuracy: 0.9500\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.1200 - accuracy: 0.9555 - val_loss: 0.3318 - val_accuracy: 0.9500\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.2663 - accuracy: 0.8921 - val_loss: 0.3835 - val_accuracy: 0.8400\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.2357 - accuracy: 0.9110 - val_loss: 0.6633 - val_accuracy: 0.8300\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.1995 - accuracy: 0.9288 - val_loss: 0.4383 - val_accuracy: 0.9000\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.1408 - accuracy: 0.9522 - val_loss: 0.3355 - val_accuracy: 0.9100\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.0974 - accuracy: 0.9666 - val_loss: 0.3983 - val_accuracy: 0.9300\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.1053 - accuracy: 0.9666 - val_loss: 0.2757 - val_accuracy: 0.9600\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.1305 - accuracy: 0.9466 - val_loss: 0.3870 - val_accuracy: 0.9100\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.1558 - accuracy: 0.9455 - val_loss: 0.3082 - val_accuracy: 0.9400\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.1054 - accuracy: 0.9677 - val_loss: 0.2831 - val_accuracy: 0.9000\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.1653 - accuracy: 0.9444 - val_loss: 0.4437 - val_accuracy: 0.8800\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.1447 - accuracy: 0.9522 - val_loss: 0.2047 - val_accuracy: 0.9500\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.1063 - accuracy: 0.9711 - val_loss: 0.3954 - val_accuracy: 0.9100\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.2065 - accuracy: 0.9422 - val_loss: 0.4349 - val_accuracy: 0.8600\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.1430 - accuracy: 0.9499 - val_loss: 0.4350 - val_accuracy: 0.9200\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.2688 - accuracy: 0.9232 - val_loss: 0.4568 - val_accuracy: 0.8600\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.1999 - accuracy: 0.9321 - val_loss: 0.4427 - val_accuracy: 0.9100\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.1209 - accuracy: 0.9577 - val_loss: 0.4143 - val_accuracy: 0.9300\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.0975 - accuracy: 0.9666 - val_loss: 0.3623 - val_accuracy: 0.9400\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.0949 - accuracy: 0.9733 - val_loss: 0.9890 - val_accuracy: 0.7100\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.5697 - accuracy: 0.8476 - val_loss: 0.7497 - val_accuracy: 0.8000\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.2523 - accuracy: 0.8999 - val_loss: 0.5501 - val_accuracy: 0.8900\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.2327 - accuracy: 0.9244 - val_loss: 0.3407 - val_accuracy: 0.9200\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.1297 - accuracy: 0.9622 - val_loss: 0.4327 - val_accuracy: 0.9000\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.1244 - accuracy: 0.9588 - val_loss: 0.2743 - val_accuracy: 0.9500\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.0787 - accuracy: 0.9733 - val_loss: 0.3157 - val_accuracy: 0.9300\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.0731 - accuracy: 0.9766 - val_loss: 0.3444 - val_accuracy: 0.9400\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.1325 - accuracy: 0.9511 - val_loss: 0.3017 - val_accuracy: 0.9200\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.0942 - accuracy: 0.9655 - val_loss: 0.2890 - val_accuracy: 0.9400\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.1105 - accuracy: 0.9666 - val_loss: 0.3777 - val_accuracy: 0.9500\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.0907 - accuracy: 0.9700 - val_loss: 0.3376 - val_accuracy: 0.9000\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.0909 - accuracy: 0.9700 - val_loss: 0.3204 - val_accuracy: 0.9500\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.0789 - accuracy: 0.9722 - val_loss: 0.2992 - val_accuracy: 0.9300\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.0594 - accuracy: 0.9789 - val_loss: 0.2733 - val_accuracy: 0.9500\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.0551 - accuracy: 0.9822 - val_loss: 0.3010 - val_accuracy: 0.9500\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.0548 - accuracy: 0.9811 - val_loss: 0.2355 - val_accuracy: 0.9500\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.0549 - accuracy: 0.9800 - val_loss: 0.2882 - val_accuracy: 0.9500\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.0540 - accuracy: 0.9800 - val_loss: 0.2972 - val_accuracy: 0.9400\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 16306)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               4174592   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 256)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 15)                3855      \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 15)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,244,239\n",
      "Trainable params: 4,244,239\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import metrics\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "def build_model(n_classes,\n",
    "                hidden_layer_sizes=[],\n",
    "                activation='relu',\n",
    "                final_layer_activation='softmax',\n",
    "                dropout=0.0,\n",
    "                optimizer='Adam',\n",
    "                learning_rate=0.01):\n",
    "  \"\"\"Build a multi-class logistic regression model using Keras.\n",
    "\n",
    "  Args:\n",
    "    n_classes: Number of output classes in the dataset.\n",
    "    hidden_layer_sizes: A list with the number of units in each hidden layer.\n",
    "    activation: The activation function to use for the hidden layers.\n",
    "    optimizer: The optimizer to use (SGD, Adam).\n",
    "    learning_rate: The desired learning rate for the optimizer.\n",
    "\n",
    "  Returns:\n",
    "    model: A tf.keras model (graph).\n",
    "  \"\"\"\n",
    "  tf.keras.backend.clear_session()\n",
    "  np.random.seed(0)\n",
    "  tf.random.set_seed(0)\n",
    "  model = keras.Sequential()\n",
    "  model.add(keras.layers.Flatten())\n",
    "\n",
    "  for hidden_layer_size in hidden_layer_sizes:\n",
    "    model.add(keras.layers.Dense(hidden_layer_size))\n",
    "    model.add(keras.layers.Activation(activation))\n",
    "    if dropout > 0:\n",
    "      model.add(keras.layers.Dropout(dropout))\n",
    "\n",
    "  model.add(keras.layers.Dense(n_classes))\n",
    "  model.add(keras.layers.Activation(final_layer_activation))\n",
    "  opt = None\n",
    "  if optimizer == 'SGD':\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "  elif optimizer == 'Adam':\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "  else:\n",
    "    raise f\"Unsupported optimizer, {optimizer}\"\n",
    "  model.compile(loss='sparse_categorical_crossentropy', \n",
    "    optimizer=opt, metrics=['accuracy'])    \n",
    "  return model\n",
    "\n",
    "def train_model(X_train, Y_train, num_classes,\n",
    "                       hidden_layer_sizes=[],\n",
    "                       activation='tanh',\n",
    "                       final_layer_activation='softmax',\n",
    "                       dropout=0.2,\n",
    "                       optimizer='Adam',\n",
    "                       learning_rate=0.01,\n",
    "                       batch_size=64,\n",
    "                       num_epochs=5):\n",
    "\n",
    "  # Build the model.\n",
    "  model = build_model(num_classes,\n",
    "                      hidden_layer_sizes=hidden_layer_sizes,\n",
    "                      activation=activation,\n",
    "                      final_layer_activation=final_layer_activation,\n",
    "                      dropout=dropout,\n",
    "                      optimizer=optimizer,\n",
    "                      learning_rate=learning_rate)\n",
    "\n",
    "  # Train the model.\n",
    "  print('Training...')\n",
    "  history = model.fit(\n",
    "    x=X_train,\n",
    "    y=Y_train,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.1,\n",
    "    verbose=1)\n",
    "\n",
    "  model.summary()\n",
    "  return model\n",
    "\n",
    "nn3 = train_model(X_tr, Y_tr['EMPO_3_int'], 15,\n",
    "    hidden_layer_sizes=[256, 256],\n",
    "    dropout=0.2,\n",
    "    optimizer='Adam',\n",
    "    learning_rate=0.01,\n",
    "    batch_size=128,\n",
    "    num_epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b48062",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a18511f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on EMPO 3: 0.8999999761581421\n"
     ]
    }
   ],
   "source": [
    "# Scoring model\n",
    "\n",
    "nn3_accuracy = nn3.evaluate(x=X_val, y=Y_val['EMPO_3_int'], verbose=0, return_dict=True)['accuracy']\n",
    "print(f\"Accuracy on EMPO 3: {nn3_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ffb1f1",
   "metadata": {},
   "source": [
    "### Retrain best model\n",
    "After experimenting with models, retrain your favorite model using entire training set (including validation) before saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e2ce65f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'EMPO_3_int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/indexes/base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'EMPO_3_int'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      3\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 5\u001b[0m nn3 \u001b[38;5;241m=\u001b[39m train_model(x_train, \u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEMPO_3_int\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;241m15\u001b[39m,\n\u001b[1;32m      6\u001b[0m     hidden_layer_sizes\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m],\n\u001b[1;32m      7\u001b[0m     dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m      8\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAdam\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m     10\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m     11\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     13\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     14\u001b[0m wallclock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(end_time \u001b[38;5;241m-\u001b[39m start_time)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/indexes/base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'EMPO_3_int'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "nn3 = train_model(x_train, y_train['EMPO_3_int'], 15,\n",
    "    hidden_layer_sizes=[256, 256],\n",
    "    dropout=0.2,\n",
    "    optimizer='Adam',\n",
    "    learning_rate=0.01,\n",
    "    batch_size=128,\n",
    "    num_epochs=100)\n",
    "\n",
    "end_time = time.time()\n",
    "wallclock = int(end_time - start_time)\n",
    "print(f\"Wallclock = {wallclock} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e55e7e",
   "metadata": {},
   "source": [
    "### Save fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d1e12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model as joblib or pkl file to 'model_joblibs' folder\n",
    "from joblib import dump\n",
    "\n",
    "dump(clf, '../model_joblibs/example_neural_network.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882732f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
