{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d4965eb",
   "metadata": {},
   "source": [
    "# Example modeling process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10664d32",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56cae12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from helpers import (get_training_observations, \n",
    "                     get_training_labels, \n",
    "                     get_protein_proportions,\n",
    "                     drop_empty_columns)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Model specific imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02264c03",
   "metadata": {},
   "source": [
    "### Load training data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "426cf755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting all training observations from 'metagenome_classification.db'...\n",
      "There are 16306 features\n",
      "Getting all training labels from 'metagenome_classification.db'...\n"
     ]
    }
   ],
   "source": [
    "x_train_raw_counts = get_training_observations()\n",
    "x_train = get_protein_proportions(x_train_raw_counts)\n",
    "print(f\"There are {x_train.shape[1]} features\")\n",
    "y_train = get_training_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f0907b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>index</th>\n",
       "      <th>PF00001.19</th>\n",
       "      <th>PF00002.22</th>\n",
       "      <th>PF00003.20</th>\n",
       "      <th>PF00004.27</th>\n",
       "      <th>PF00005.25</th>\n",
       "      <th>PF00006.23</th>\n",
       "      <th>PF00007.20</th>\n",
       "      <th>PF00008.25</th>\n",
       "      <th>PF00009.25</th>\n",
       "      <th>PF00010.24</th>\n",
       "      <th>...</th>\n",
       "      <th>PF17216.1</th>\n",
       "      <th>PF17217.1</th>\n",
       "      <th>PF17218.1</th>\n",
       "      <th>PF17219.1</th>\n",
       "      <th>PF17220.1</th>\n",
       "      <th>PF17221.1</th>\n",
       "      <th>PF17222.1</th>\n",
       "      <th>PF17223.1</th>\n",
       "      <th>PF17224.1</th>\n",
       "      <th>PF17225.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004020</td>\n",
       "      <td>0.006243</td>\n",
       "      <td>0.001039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003575</td>\n",
       "      <td>0.013739</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002235</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.376880e-07</td>\n",
       "      <td>2.188440e-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001619</td>\n",
       "      <td>0.016218</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.001711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002737</td>\n",
       "      <td>0.019874</td>\n",
       "      <td>0.001785</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001992</td>\n",
       "      <td>0.012389</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.016435</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001806</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001404</td>\n",
       "      <td>0.023274</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002301</td>\n",
       "      <td>0.015752</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.014797</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001689</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004574</td>\n",
       "      <td>0.013601</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002402</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 16306 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "index    PF00001.19    PF00002.22  PF00003.20  PF00004.27  PF00005.25  \\\n",
       "0      0.000000e+00  0.000000e+00         0.0    0.004020    0.006243   \n",
       "1      0.000000e+00  0.000000e+00         0.0    0.003575    0.013739   \n",
       "2      4.376880e-07  2.188440e-07         0.0    0.001619    0.016218   \n",
       "3      0.000000e+00  0.000000e+00         0.0    0.002737    0.019874   \n",
       "4      0.000000e+00  0.000000e+00         0.0    0.001992    0.012389   \n",
       "5      0.000000e+00  0.000000e+00         0.0    0.001684    0.016435   \n",
       "6      0.000000e+00  0.000000e+00         0.0    0.001404    0.023274   \n",
       "7      0.000000e+00  0.000000e+00         0.0    0.002301    0.015752   \n",
       "8      0.000000e+00  0.000000e+00         0.0    0.001885    0.014797   \n",
       "9      0.000000e+00  0.000000e+00         0.0    0.004574    0.013601   \n",
       "\n",
       "index  PF00006.23  PF00007.20  PF00008.25  PF00009.25  PF00010.24  ...  \\\n",
       "0        0.001039         0.0    0.000000    0.003265    0.000000  ...   \n",
       "1        0.001026         0.0    0.000000    0.002235    0.000007  ...   \n",
       "2        0.000916         0.0    0.000002    0.001711    0.000000  ...   \n",
       "3        0.001785         0.0    0.000000    0.003570    0.000000  ...   \n",
       "4        0.001154         0.0    0.000013    0.002286    0.000000  ...   \n",
       "5        0.000931         0.0    0.000000    0.001806    0.000000  ...   \n",
       "6        0.000845         0.0    0.000000    0.001497    0.000000  ...   \n",
       "7        0.000925         0.0    0.000000    0.001843    0.000000  ...   \n",
       "8        0.001012         0.0    0.000000    0.001689    0.000000  ...   \n",
       "9        0.000995         0.0    0.000000    0.002402    0.000000  ...   \n",
       "\n",
       "index  PF17216.1  PF17217.1  PF17218.1  PF17219.1  PF17220.1  PF17221.1  \\\n",
       "0            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "1            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "2            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "3            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "4            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "5            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "6            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "7            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "8            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "9            0.0        0.0        0.0        0.0        0.0        0.0   \n",
       "\n",
       "index  PF17222.1  PF17223.1  PF17224.1  PF17225.1  \n",
       "0            0.0        0.0        0.0        0.0  \n",
       "1            0.0        0.0        0.0        0.0  \n",
       "2            0.0        0.0        0.0        0.0  \n",
       "3            0.0        0.0        0.0        0.0  \n",
       "4            0.0        0.0        0.0        0.0  \n",
       "5            0.0        0.0        0.0        0.0  \n",
       "6            0.0        0.0        0.0        0.0  \n",
       "7            0.0        0.0        0.0        0.0  \n",
       "8            0.0        0.0        0.0        0.0  \n",
       "9            0.0        0.0        0.0        0.0  \n",
       "\n",
       "[10 rows x 16306 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc49d270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>index</th>\n",
       "      <th>EMPO_1</th>\n",
       "      <th>EMPO_2</th>\n",
       "      <th>EMPO_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Free-living</td>\n",
       "      <td>Saline</td>\n",
       "      <td>Hypersaline (saline)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Free-living</td>\n",
       "      <td>Saline</td>\n",
       "      <td>Water (saline)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Host-associated</td>\n",
       "      <td>Plant</td>\n",
       "      <td>Plant rhizosphere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Free-living</td>\n",
       "      <td>Non-saline</td>\n",
       "      <td>Soil (non-saline)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Free-living</td>\n",
       "      <td>Saline</td>\n",
       "      <td>Water (saline)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "index           EMPO_1      EMPO_2                EMPO_3\n",
       "0          Free-living      Saline  Hypersaline (saline)\n",
       "1          Free-living      Saline        Water (saline)\n",
       "2      Host-associated       Plant     Plant rhizosphere\n",
       "3          Free-living  Non-saline     Soil (non-saline)\n",
       "4          Free-living      Saline        Water (saline)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f397441",
   "metadata": {},
   "source": [
    "### Data manipulation\n",
    "Validation splits, dimensionality reduction, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eac6bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train/validation if not CV\n",
    "X_tr, X_val, Y_tr, Y_val = train_test_split(x_train, y_train, test_size=0.2) #, random_state=1)\n",
    "\n",
    "# Dimensionality reduction?\n",
    "svd = TruncatedSVD(n_components=100, n_iter=7, random_state=42)\n",
    "svd.fit(X_tr)\n",
    "\n",
    "new_x_train = svd.transform(X_tr)\n",
    "new_x_val = svd.transform(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c051696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert string labels to numeric\n",
    "labels3 = [\n",
    "    'Aerosol (non-saline)',\n",
    "    'Animal corpus',\n",
    "    'Animal proximal gut',\n",
    "    'Hypersaline (saline)',\n",
    "    'Plant corpus',\n",
    "    'Plant rhizosphere',\n",
    "    'Plant surface',\n",
    "    'Sediment (non-saline)',\n",
    "    'Sediment (saline)',\n",
    "    'Soil (non-saline)',\n",
    "    'Subsurface (non-saline)',\n",
    "    'Surface (non-saline)',\n",
    "    'Surface (saline)',\n",
    "    'Water (non-saline)',\n",
    "    'Water (saline)'\n",
    "]\n",
    "labels3_map = {}\n",
    "for i in range(0,len(labels3)):\n",
    "    label = labels3[i]\n",
    "    labels3_map[label] = i\n",
    "\n",
    "Y_tr['EMPO_3_int'] = Y_tr['EMPO_3'].map(labels3_map)\n",
    "Y_tr.head()\n",
    "Y_val['EMPO_3_int'] = Y_val['EMPO_3'].map(labels3_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bb1981",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "22269d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch 1/100\n",
      "8/8 [==============================] - 1s 51ms/step - loss: 2.2158 - accuracy: 0.2670 - val_loss: 2.2361 - val_accuracy: 0.1700\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 1.7947 - accuracy: 0.3715 - val_loss: 1.7115 - val_accuracy: 0.4100\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 1.3975 - accuracy: 0.5128 - val_loss: 1.5854 - val_accuracy: 0.6000\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 1.3142 - accuracy: 0.5695 - val_loss: 1.1378 - val_accuracy: 0.7000\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 51ms/step - loss: 1.0229 - accuracy: 0.6974 - val_loss: 1.1477 - val_accuracy: 0.7200\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.9939 - accuracy: 0.6986 - val_loss: 0.9005 - val_accuracy: 0.7600\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.8538 - accuracy: 0.7319 - val_loss: 0.7325 - val_accuracy: 0.7800\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.7364 - accuracy: 0.7475 - val_loss: 0.7150 - val_accuracy: 0.7900\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.7207 - accuracy: 0.7742 - val_loss: 0.7426 - val_accuracy: 0.7900\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.6542 - accuracy: 0.7853 - val_loss: 0.7502 - val_accuracy: 0.7600\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.5783 - accuracy: 0.7964 - val_loss: 0.6194 - val_accuracy: 0.8100\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.5605 - accuracy: 0.8042 - val_loss: 0.6422 - val_accuracy: 0.8200\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.5461 - accuracy: 0.8053 - val_loss: 0.5791 - val_accuracy: 0.8400\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.4269 - accuracy: 0.8521 - val_loss: 0.5431 - val_accuracy: 0.8700\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.4106 - accuracy: 0.8665 - val_loss: 0.6069 - val_accuracy: 0.8200\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.4452 - accuracy: 0.8532 - val_loss: 0.5297 - val_accuracy: 0.8100\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.4961 - accuracy: 0.8131 - val_loss: 0.5656 - val_accuracy: 0.8100\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.4468 - accuracy: 0.8610 - val_loss: 0.5710 - val_accuracy: 0.8800\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.3947 - accuracy: 0.8621 - val_loss: 0.8708 - val_accuracy: 0.7700\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.5714 - accuracy: 0.8065 - val_loss: 1.1394 - val_accuracy: 0.6900\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.6889 - accuracy: 0.7720 - val_loss: 0.7032 - val_accuracy: 0.8200\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.5157 - accuracy: 0.8242 - val_loss: 0.8061 - val_accuracy: 0.7900\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.4088 - accuracy: 0.8676 - val_loss: 0.6415 - val_accuracy: 0.8500\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.4032 - accuracy: 0.8710 - val_loss: 0.5380 - val_accuracy: 0.8500\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.3335 - accuracy: 0.8932 - val_loss: 0.4965 - val_accuracy: 0.8500\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.3060 - accuracy: 0.8943 - val_loss: 0.4451 - val_accuracy: 0.8900\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.2914 - accuracy: 0.9132 - val_loss: 0.4914 - val_accuracy: 0.8600\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.2896 - accuracy: 0.8943 - val_loss: 0.5122 - val_accuracy: 0.8800\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.3165 - accuracy: 0.8754 - val_loss: 0.4524 - val_accuracy: 0.9200\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.2717 - accuracy: 0.9077 - val_loss: 0.4127 - val_accuracy: 0.8900\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.2294 - accuracy: 0.9210 - val_loss: 0.4614 - val_accuracy: 0.8900\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.2041 - accuracy: 0.9344 - val_loss: 0.4915 - val_accuracy: 0.8800\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.2589 - accuracy: 0.9143 - val_loss: 0.4596 - val_accuracy: 0.8800\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.3657 - accuracy: 0.8765 - val_loss: 0.5911 - val_accuracy: 0.8100\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.2592 - accuracy: 0.9210 - val_loss: 0.4560 - val_accuracy: 0.9000\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.2238 - accuracy: 0.9266 - val_loss: 0.5387 - val_accuracy: 0.9000\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.2743 - accuracy: 0.9110 - val_loss: 0.3714 - val_accuracy: 0.8900\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.2626 - accuracy: 0.9055 - val_loss: 0.6503 - val_accuracy: 0.8400\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.2467 - accuracy: 0.9155 - val_loss: 0.4879 - val_accuracy: 0.8900\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.2293 - accuracy: 0.9188 - val_loss: 0.3828 - val_accuracy: 0.9100\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.2044 - accuracy: 0.9266 - val_loss: 0.4447 - val_accuracy: 0.8900\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.1620 - accuracy: 0.9499 - val_loss: 0.5260 - val_accuracy: 0.8500\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.3431 - accuracy: 0.8732 - val_loss: 0.5496 - val_accuracy: 0.8700\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.2784 - accuracy: 0.9066 - val_loss: 0.4076 - val_accuracy: 0.9200\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 44ms/step - loss: 0.1824 - accuracy: 0.9422 - val_loss: 0.4502 - val_accuracy: 0.9000\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.1580 - accuracy: 0.9499 - val_loss: 0.4319 - val_accuracy: 0.9100\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.1733 - accuracy: 0.9399 - val_loss: 0.5858 - val_accuracy: 0.8600\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.1936 - accuracy: 0.9310 - val_loss: 0.4395 - val_accuracy: 0.9100\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.1656 - accuracy: 0.9388 - val_loss: 0.5383 - val_accuracy: 0.8700\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.1830 - accuracy: 0.9377 - val_loss: 0.5314 - val_accuracy: 0.8800\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.1456 - accuracy: 0.9466 - val_loss: 0.4575 - val_accuracy: 0.9000\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.1384 - accuracy: 0.9533 - val_loss: 0.5367 - val_accuracy: 0.9100\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.1283 - accuracy: 0.9544 - val_loss: 0.4732 - val_accuracy: 0.9100\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.1288 - accuracy: 0.9588 - val_loss: 0.4931 - val_accuracy: 0.9200\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.1067 - accuracy: 0.9600 - val_loss: 0.4507 - val_accuracy: 0.9200\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.0950 - accuracy: 0.9633 - val_loss: 0.6953 - val_accuracy: 0.8500\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.3963 - accuracy: 0.8721 - val_loss: 0.8908 - val_accuracy: 0.8000\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.2513 - accuracy: 0.9177 - val_loss: 0.5452 - val_accuracy: 0.8600\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 39ms/step - loss: 0.2422 - accuracy: 0.9210 - val_loss: 0.5508 - val_accuracy: 0.8900\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.1691 - accuracy: 0.9433 - val_loss: 0.4714 - val_accuracy: 0.9000\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.1176 - accuracy: 0.9588 - val_loss: 0.4723 - val_accuracy: 0.9100\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.1123 - accuracy: 0.9611 - val_loss: 0.4344 - val_accuracy: 0.9100\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.1006 - accuracy: 0.9644 - val_loss: 0.4683 - val_accuracy: 0.9200\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.1000 - accuracy: 0.9600 - val_loss: 0.4387 - val_accuracy: 0.9100\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.0944 - accuracy: 0.9700 - val_loss: 0.5332 - val_accuracy: 0.9300\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.1927 - accuracy: 0.9355 - val_loss: 0.5086 - val_accuracy: 0.8700\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.1361 - accuracy: 0.9455 - val_loss: 0.6615 - val_accuracy: 0.8800\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.3066 - accuracy: 0.9055 - val_loss: 0.6575 - val_accuracy: 0.8800\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.1618 - accuracy: 0.9310 - val_loss: 0.5981 - val_accuracy: 0.8300\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 33ms/step - loss: 0.1339 - accuracy: 0.9488 - val_loss: 0.7094 - val_accuracy: 0.9100\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 36ms/step - loss: 0.1169 - accuracy: 0.9566 - val_loss: 0.4679 - val_accuracy: 0.9000\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.1032 - accuracy: 0.9655 - val_loss: 0.5325 - val_accuracy: 0.9100\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.0985 - accuracy: 0.9633 - val_loss: 0.5204 - val_accuracy: 0.9100\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.1118 - accuracy: 0.9566 - val_loss: 0.5055 - val_accuracy: 0.8800\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 34ms/step - loss: 0.1160 - accuracy: 0.9544 - val_loss: 0.4788 - val_accuracy: 0.9300\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.0816 - accuracy: 0.9700 - val_loss: 0.5608 - val_accuracy: 0.9100\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.0854 - accuracy: 0.9689 - val_loss: 0.8910 - val_accuracy: 0.7800\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 37ms/step - loss: 0.4908 - accuracy: 0.8699 - val_loss: 0.7864 - val_accuracy: 0.8700\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.2750 - accuracy: 0.9155 - val_loss: 0.5814 - val_accuracy: 0.9000\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.1823 - accuracy: 0.9333 - val_loss: 0.5257 - val_accuracy: 0.9300\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.1258 - accuracy: 0.9455 - val_loss: 0.5498 - val_accuracy: 0.9100\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.0981 - accuracy: 0.9600 - val_loss: 0.5268 - val_accuracy: 0.9200\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.0811 - accuracy: 0.9689 - val_loss: 0.4797 - val_accuracy: 0.9100\n",
      "Epoch 84/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.0850 - accuracy: 0.9733 - val_loss: 0.5901 - val_accuracy: 0.9100\n",
      "Epoch 85/100\n",
      "8/8 [==============================] - 0s 47ms/step - loss: 0.0645 - accuracy: 0.9766 - val_loss: 0.5250 - val_accuracy: 0.9200\n",
      "Epoch 86/100\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.0783 - accuracy: 0.9722 - val_loss: 0.4801 - val_accuracy: 0.9300\n",
      "Epoch 87/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.0533 - accuracy: 0.9822 - val_loss: 0.5469 - val_accuracy: 0.9200\n",
      "Epoch 88/100\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.0541 - accuracy: 0.9833 - val_loss: 0.5206 - val_accuracy: 0.9100\n",
      "Epoch 89/100\n",
      "8/8 [==============================] - 0s 45ms/step - loss: 0.0527 - accuracy: 0.9800 - val_loss: 0.5442 - val_accuracy: 0.9200\n",
      "Epoch 90/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.0576 - accuracy: 0.9800 - val_loss: 0.5501 - val_accuracy: 0.9200\n",
      "Epoch 91/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.0503 - accuracy: 0.9833 - val_loss: 0.5265 - val_accuracy: 0.9200\n",
      "Epoch 92/100\n",
      "8/8 [==============================] - 0s 46ms/step - loss: 0.0506 - accuracy: 0.9822 - val_loss: 0.5518 - val_accuracy: 0.9100\n",
      "Epoch 93/100\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.0496 - accuracy: 0.9811 - val_loss: 0.5751 - val_accuracy: 0.9100\n",
      "Epoch 94/100\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.0610 - accuracy: 0.9811 - val_loss: 0.6018 - val_accuracy: 0.9100\n",
      "Epoch 95/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.0581 - accuracy: 0.9822 - val_loss: 0.5143 - val_accuracy: 0.9300\n",
      "Epoch 96/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.0527 - accuracy: 0.9844 - val_loss: 0.5738 - val_accuracy: 0.9100\n",
      "Epoch 97/100\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.0498 - accuracy: 0.9822 - val_loss: 0.6010 - val_accuracy: 0.9200\n",
      "Epoch 98/100\n",
      "8/8 [==============================] - 0s 35ms/step - loss: 0.0488 - accuracy: 0.9766 - val_loss: 0.6016 - val_accuracy: 0.9200\n",
      "Epoch 99/100\n",
      "8/8 [==============================] - 0s 38ms/step - loss: 0.0449 - accuracy: 0.9833 - val_loss: 0.6259 - val_accuracy: 0.9000\n",
      "Epoch 100/100\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.0432 - accuracy: 0.9844 - val_loss: 0.6378 - val_accuracy: 0.9200\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 16306)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               4174592   \n",
      "                                                                 \n",
      " activation (Activation)     (None, 256)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 256)               65792     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 15)                3855      \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 15)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,244,239\n",
      "Trainable params: 4,244,239\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import metrics\n",
    "tf.get_logger().setLevel('INFO')\n",
    "\n",
    "def build_model(n_classes,\n",
    "                hidden_layer_sizes=[],\n",
    "                activation='relu',\n",
    "                final_layer_activation='softmax',\n",
    "                dropout=0.0,\n",
    "                optimizer='Adam',\n",
    "                learning_rate=0.01):\n",
    "  \"\"\"Build a multi-class logistic regression model using Keras.\n",
    "\n",
    "  Args:\n",
    "    n_classes: Number of output classes in the dataset.\n",
    "    hidden_layer_sizes: A list with the number of units in each hidden layer.\n",
    "    activation: The activation function to use for the hidden layers.\n",
    "    optimizer: The optimizer to use (SGD, Adam).\n",
    "    learning_rate: The desired learning rate for the optimizer.\n",
    "\n",
    "  Returns:\n",
    "    model: A tf.keras model (graph).\n",
    "  \"\"\"\n",
    "  tf.keras.backend.clear_session()\n",
    "  np.random.seed(0)\n",
    "  tf.random.set_seed(0)\n",
    "  model = keras.Sequential()\n",
    "  model.add(keras.layers.Flatten())\n",
    "\n",
    "  for hidden_layer_size in hidden_layer_sizes:\n",
    "    model.add(keras.layers.Dense(hidden_layer_size))\n",
    "    model.add(keras.layers.Activation(activation))\n",
    "    if dropout > 0:\n",
    "      model.add(keras.layers.Dropout(dropout))\n",
    "\n",
    "  model.add(keras.layers.Dense(n_classes))\n",
    "  model.add(keras.layers.Activation(final_layer_activation))\n",
    "  opt = None\n",
    "  if optimizer == 'SGD':\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "  elif optimizer == 'Adam':\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "  else:\n",
    "    raise f\"Unsupported optimizer, {optimizer}\"\n",
    "  model.compile(loss='sparse_categorical_crossentropy', \n",
    "    optimizer=opt, metrics=['accuracy'])    \n",
    "  return model\n",
    "\n",
    "def train_model(X_train, Y_train, num_classes,\n",
    "                       hidden_layer_sizes=[],\n",
    "                       activation='tanh',\n",
    "                       final_layer_activation='softmax',\n",
    "                       dropout=0.2,\n",
    "                       optimizer='Adam',\n",
    "                       learning_rate=0.01,\n",
    "                       batch_size=64,\n",
    "                       num_epochs=5):\n",
    "\n",
    "  # Build the model.\n",
    "  model = build_model(num_classes,\n",
    "                      hidden_layer_sizes=hidden_layer_sizes,\n",
    "                      activation=activation,\n",
    "                      final_layer_activation=final_layer_activation,\n",
    "                      dropout=dropout,\n",
    "                      optimizer=optimizer,\n",
    "                      learning_rate=learning_rate)\n",
    "\n",
    "  # Train the model.\n",
    "  print('Training...')\n",
    "  history = model.fit(\n",
    "    x=X_train,\n",
    "    y=Y_train,\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.1,\n",
    "    verbose=1)\n",
    "\n",
    "  model.summary()\n",
    "  return model\n",
    "\n",
    "nn3 = train_model(X_tr, Y_tr['EMPO_3_int'], 15,\n",
    "    hidden_layer_sizes=[256, 256],\n",
    "    dropout=0.2,\n",
    "    optimizer='Adam',\n",
    "    learning_rate=0.01,\n",
    "    batch_size=128,\n",
    "    num_epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b48062",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a18511f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on EMPO 3: 0.9359999895095825\n"
     ]
    }
   ],
   "source": [
    "# Scoring model\n",
    "\n",
    "nn3_accuracy = nn3.evaluate(x=X_val, y=Y_val['EMPO_3_int'], verbose=0, return_dict=True)['accuracy']\n",
    "print(f\"Accuracy on EMPO 3: {nn3_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ffb1f1",
   "metadata": {},
   "source": [
    "### Retrain best model\n",
    "After experimenting with models, retrain your favorite model using entire training set (including validation) before saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2ce65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "#clf = LogisticRegression(multi_class='multinomial').fit(x_train, y_train['EMPO_1'])\n",
    "\n",
    "end_time = time.time()\n",
    "wallclock = int(end_time - start_time)\n",
    "print(f\"Wallclock = {wallclock} sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e55e7e",
   "metadata": {},
   "source": [
    "### Save fitted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d1e12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best model as joblib or pkl file to 'model_joblibs' folder\n",
    "from joblib import dump\n",
    "\n",
    "dump(clf, '../model_joblibs/example_neural_network.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882732f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
